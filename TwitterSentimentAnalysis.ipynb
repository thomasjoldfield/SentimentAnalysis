{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tweepy\n",
    "import json\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "from config import api_twitter\n",
    "from config import api_twitter_secret\n",
    "from config import api_access_token\n",
    "from config import api_access_token_secret\n",
    "# Your Twitter API Keys\n",
    "consumer_key = api_twitter\n",
    "consumer_secret = api_twitter_secret\n",
    "access_token = api_access_token\n",
    "access_token_secret = api_access_token_secret\n",
    "\n",
    "# Setup Tweepy API Authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's keep our twitter targets in a list\n",
    "target_list = [\"@bbc\", \"@cnn\"]\n",
    "comp_dict = {}\n",
    "pos_dict = {}\n",
    "neg_dict = {}\n",
    "neu_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just to make sure it's working....\n",
    "#tweets = api.user_timeline(target_list[0])\n",
    "#print(tweets)\n",
    "#tweets[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Your final Jupyter notebook must:\n",
    "\n",
    "Pull last 100 tweets from each outlet.\n",
    "Perform a sentiment analysis with the compound, positive, neutral, and negative scoring for each tweet.\n",
    "Pull into a DataFrame the tweet's source acount, its text, its date, and its compound, positive, neutral, and negative sentiment scores.\n",
    "Export the data in the DataFrame into a CSV file.\n",
    "Save PNG images for each plot.'''\n",
    "\n",
    "\n",
    "#let's create a function series of functions to do this.\n",
    "def compoundanalysis(handle):\n",
    "    print(\"Running compound analysis for {}\".format(handle))\n",
    "    compound_list = []\n",
    "    oldest_tweet = None\n",
    "    for x in range (0,5):\n",
    "        tweet_search = api.user_timeline(handle, max_id = oldest_tweet)\n",
    "        for tweet in tweet_search:\n",
    "            results = analyzer.polarity_scores(tweet[\"text\"])\n",
    "            compound_list.append(results[\"compound\"])\n",
    "            oldest_tweet = tweet[\"id\"]\n",
    "    return(compound_list)\n",
    "\n",
    "#Damn, ok, that wasn't so bad. That return really opens up some good stuff. Let's create 3 more for other returns\n",
    "    \n",
    "def positiveanalysis(handle):\n",
    "    print(\"Running positive analysis for {}\".format(handle))\n",
    "    positive_list = []\n",
    "    oldest_tweet = None\n",
    "    for x in range (0,5):\n",
    "        tweet_search = api.user_timeline(handle, max_id = oldest_tweet)\n",
    "        for tweet in tweet_search:\n",
    "            results = analyzer.polarity_scores(tweet[\"text\"])\n",
    "            positive_list.append(results[\"pos\"])\n",
    "            oldest_tweet = tweet[\"id\"]\n",
    "    return(positive_list)\n",
    "\n",
    "def negativeanalysis(handle):\n",
    "    print(\"Running negative analysis for {}\".format(handle))\n",
    "    negative_list = []\n",
    "    oldest_tweet = None\n",
    "    for x in range (0,5):\n",
    "        tweet_search = api.user_timeline(handle, max_id = oldest_tweet)\n",
    "        for tweet in tweet_search:\n",
    "            results = analyzer.polarity_scores(tweet[\"text\"])\n",
    "            negative_list.append(results[\"neg\"])\n",
    "            oldest_tweet = tweet[\"id\"]\n",
    "    return(negative_list)\n",
    "\n",
    "def neutralanalysis(handle):\n",
    "    print(\"Running neutral analysis for {}\".format(handle))\n",
    "    neutral_list = []\n",
    "    oldest_tweet = None\n",
    "    for x in range (0,5):\n",
    "        tweet_search = api.user_timeline(handle, max_id = oldest_tweet)\n",
    "        for tweet in tweet_search:\n",
    "            results = analyzer.polarity_scores(tweet[\"text\"])\n",
    "            neutral_list.append(results[\"neu\"])\n",
    "            oldest_tweet = tweet[\"id\"]\n",
    "    return(neutral_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running compound analysis for @bbc\n",
      "Running positive analysis for @bbc\n",
      "Running negative analysis for @bbc\n",
      "Running neutral analysis for @bbc\n",
      "Running compound analysis for @cnn\n",
      "Running positive analysis for @cnn\n"
     ]
    }
   ],
   "source": [
    "#Then we just do this, and build a dictionary! Ain't no thing.\n",
    "for handle in target_list:\n",
    "    comp_dict[\"comp_\" + handle] = compoundanalysis(handle)\n",
    "    pos_dict[\"pos_\"+handle] = positiveanalysis(handle)\n",
    "    neg_dict[\"neg_\"+handle] = negativeanalysis(handle)\n",
    "    neu_dict[\"neu_\"+handle] = neutralanalysis(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Turn it all into a dataframe by taking a list of those dictionaries\n",
    "tweet_df = py.DataFrame([comp_dict],[pos_dict],[neg_dict],[neu_dict])\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([0.0, 0.4939, 0.25, 0.0, 0.0, -0.2263, 0.0, 0.0, -0.2023, 0.875, 0.2732, -0.0772, 0.5859, 0.3818, 0.0, 0.0, 0.6369, 0.3612, 0.5719, 0.0, 0.0, -0.7269, 0.6515, 0.6908, 0.0, 0.0, -0.5719, -0.296, 0.0, 0.0, 0.69, 0.0, 0.0, -0.8658, 0.0, 0.34, 0.4588, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1531, -0.7777, 0.6037, 0.8126, 0.0, 0.0, 0.5499, -0.5849, 0.0, -0.4215, 0.0, 0.2023, 0.8617, 0.0, 0.0, 0.0, 0.0, 0.25, 0.25, 0.0, 0.4579, -0.6908, 0.7096, 0.0, -0.765, 0.0, 0.0, 0.0028, 0.4588, -0.2263, 0.0, 0.0, 0.4215, 0.6027, 0.0, 0.2023, 0.3164, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
